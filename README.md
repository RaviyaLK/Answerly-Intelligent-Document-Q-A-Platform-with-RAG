ğŸ§  Intelligent Document Q&A Platform with RAG


ğŸ“˜ Overview
This project demonstrates a Retrieval-Augmented Generation (RAG) system that allows users to upload documents and ask questions, receiving contextually relevant answers powered by Large Language Models (LLMs).

ğŸš€ Features
Document upload and text extraction

Text chunking and embedding using sentence-transformers

Vector storage with FAISS

Question answering via Hugging Face Transformers

Backend API built with FastAPI

Frontend interface using Next.js

CI/CD pipeline with GitHub Actions

Deployment on AWS SageMaker (Free Tier)

Metadata storage in Snowflake (Free Trial)

ğŸ› ï¸ Tech Stack
Frontend: Next.js, Tailwind CSS

Backend: FastAPI, Python

LLM: Hugging Face Transformers

Embeddings: sentence-transformers

Vector Store: FAISS

CI/CD: GitHub Actions

Deployment: Docker, AWS SageMaker

Database: Snowflake

ğŸ“‚ Project Structure
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ ingestion.py
â”‚   â”œâ”€â”€ embedding.py
â”‚   â”œâ”€â”€ qa_pipeline.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ (Next.js app files)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sample_pdfs/
â”‚   â””â”€â”€ chunked_data.json
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci-cd.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md


ğŸ§ª Getting Started
1. Clone the repository:
git clone https://github.com/yourusername/rag-qa-platform.git
cd rag-qa-platform

2. Set up the backend
cd backend
pip install -r requirements.txt
uvicorn main:app --reload

3. Set up the frontend:
cd frontend
npm install
npm run dev

ğŸ“¸ Screenshots